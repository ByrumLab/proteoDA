---
title: "Design and Contrast Matrix Testing"
output:
editor_options: 
  chunk_output_type: console
---

```{r setup, message=FALSE, warning=FALSE, echo = F}
knitr::opts_chunk$set(warning = F, message = F, fig.align = "center", cache = T, echo = T)
options(width = 10000)
# Load packages -----------------------------------------------------------
library(knitr)
library(waldo)
library(devtools)
library(dplyr)
load_all()
```

As I implement the model fitting parts of the R package, I've been working to better understand the contrast and design matrix parts of the limma model fitting process. Much of this was motivated by figuring out how best to deal with multiple predictor variables, and in particular multiple predictor variables when we want to know their effects (and don't just want to control for one variable while examining another). 

## Background

Charity sent [this helpful paper](https://f1000research.com/articles/9-1444/v1), which is good background for this. There is one important issue from that paper in particular: intercept- vs. no-intercept parameterizations of categorical models. Consider a experiment with a control group and a treatment group, for which we want to find DE. In the intercept parameterization, we'd estimate the intercept as the mean expression for the "default" group (probably the control), and for the non-default group we'd estimate the difference/offset from the default group. So, we're directly estimating the quantity we're interested in (differential expression). 

In the no-intercept parameterization, we estimate the mean expression for each group, and neither is the default. Then, to calculate differential expression, we need to supply a contrast matrix, telling limma how to add/subtract the per-group means to calculate differential expression (e.g., treatment minus control). This might seem like extra steps, but it is useful in cases where the predictor variable has multiple levels (e.g., A, B, and C). In the no-intercept model, we can just compare the per-group means for each pairwise comparison ($B-A$, $B-C$, etc.), which is easy to specify. In the intercept model, this is a little more complicated: comparisons against the default (say, A) are just the estimated coefficients, while comparisons between the non-default categories have to be subtracted (e.g., the B-A contrast is just $B$, but the C-B contrast is $C-B$). 

## Multiple predictors of interest

With that background out of the way, let's investigate what to do when we have multiple predictors that we're interested in. Duah first mentioned this issue to me with the Kaul project, so I'll work through it as an example. 

First, we can load in the expression data and the sample data and process it:

```{r}
# Load expression data
raw_data <- extract_data("for_testing/Example Data/kaul/Samples Report of Kaul_030922.csv",
                         pipe = "DIA",
                         enrich = "protein")

targets <- make_targets(file = "for_testing/Example Data/kaul/Kaul_030922_metafile_2_DIA.csv",
                        sampleIDs = colnames(raw_data$data),
                        pipe = "DIA",
                        enrich = "protein")

# Subset targets
sub <- subset_targets(targets = targets,
                      filter_list = list(group = "pool"))

# Process data
norm <- process_data(data = raw_data$data,
                     targets = sub$targets,
                     min.reps = 5,
                     min.grps = 2)

```

The Kaul data has two predictor variables: group (with levels of "control" and "HCV_HCC"), and gender (with levels of "male" and "female". In the metadata/targets file, we also have a third column, "group_gender", where the two predictors are combined to make one factor with 4 levels (Control.male, Control.female, etc...). Let's say we're interested in both effects: we want genes that are DE between treatments (while controlling for gender), and are also interested in genes that are DE between genders (while controlling for treatment). There are a few different ways we could maybe do it.

### The single-factor approach

One of the suggestions in the paper above is to turn your 2x2 factorial design into a 1x4 design, and then specify contrasts to compare across the original values. We've already got our 1x4 column, "group_gender". So, we'd make our design matrix like so:

```{r onecol design}
design_onecol <- make_design(targets=norm$targets,
                             group_column = "group_gender",
                             factor_columns = NULL,
                             paired_column = NULL)
```

Leading to this design matrix:

```{r}
head(design_onecol$design)
```

And this formula:

```{r}
design_onecol$designformula
```

Next, we need to specify the contrasts we're interested in. This is a little bit complicated:

```{r}
contrasts_onecol <- make_contrasts(file = "for_testing/Example Data/kaul/contrasts_onecol.csv", 
                                   design = design_onecol$design)

writeLines(contrasts_onecol$contrast.vec)
```

So here, I specified 6 different contrasts: one to look at overall differences across treatments, one across gender, and contrasts to look at the effects of treatment within each gender, and the results of gender within each treatment. This model is, implicitly, an interaction model: the effect of treatment can vary across genders (and the effect of gender can vary across treatments). 

We can fit the model and extract the DE results for our 6 contrasts:

```{r}
fit_onecol <- fit_limma_model(norm$normList[["vsn"]], 
                              design_obj = design_onecol, 
                              contrasts_obj = contrasts_onecol)
results_onecol <- extract_limma_DE_results(fit_onecol)
```

We'll return to those later. On to another couple possible ways of parameterizing things. 


### No intercept, group-first

We could just specify our two factors in our `make_design()` code:

```{r}
design_groupfirst <- make_design(targets=norm$targets,
                                 group_column = "group",
                                 factor_columns = "gender",
                                 paired_column = NULL)
```

This does lead to one issue: R's design matrices only do no-intercept encoding for the first variable. Later variables will be encoded in the intercept way:

```{r}
head(design_groupfirst$design)
```

Attempts to get around this (e.g., by making a custom design matrix with 4 columns, instead of three) lead to a matrix that is not of full rank such that limma can't fit a model. 

So, we have our two predictors encoded slightly differently: experimental group with no-intercept encoding, while gender has intercept encoding. This is implicitly an additive model: the effect of female is constant across treatments. 

Here, we don't actually need to set up contrasts for the effect of sex: they're already estimated by the model (the female term in the model is giving the contrast against the default of "male"). So, we don't actually need to specify contrasts to look at the effect of sex, we can just extract the DE results. I'll use the same code as what we do within our package functions: 

```{r}
fit_groupfirst_nocontrasts <- limma::lmFit(norm$normList[["vsn"]], design = design_groupfirst$design)
results_groupfirst_nocontrasts <- extract_limma_DE_results(list(eBayes_fit = limma::eBayes(fit_groupfirst_nocontrasts, 
                                                                                           robust = T)))
```

To calculate DE across our other predictor, group, we still need to supply contrasts. I tried a couple different ways of specifying them. With our scripts, we also need to supply a contrast to use for sex: otherwise the results for it will not be output (our code only outputs the contrasts that we ask for). For sex, this is, I think, as simple as just supplying the column name as the contrast, but I tried a couple other contrast strings as well: 

```{r}
contrasts_groupfirst <- make_contrasts(file = "for_testing/Example Data/kaul/contrasts_groupfirst.csv", 
                                       design = design_groupfirst$design)
writeLines(contrasts_groupfirst$contrast.vec)
```

Then, we can fit the model with these contrasts:

```{r}
fit_groupfirst_contrasts <- fit_limma_model(norm$normList[["vsn"]], 
                                            design_obj = design_groupfirst, 
                                            contrasts_obj = contrasts_groupfirst)
results_groupfirst_contrasts <- extract_limma_DE_results(fit_groupfirst_contrasts)
```

### No intercept, gender first

Of course, we could flip the order: use the no-intercept encoding for gender, and the intercept coding for group. This will hopefully give us the same results as above. 

```{r}
design_genderfirst <- make_design(targets=norm$targets,
                                  group_column = "gender",
                                  factor_columns = c("group"),
                                  paired_column = NULL)
# Again, can just fit it, without setting up contrasts
fit_genderfirst_nocontrasts <- limma::lmFit(norm$normList[["vsn"]], design = design_genderfirst$design)
results_genderfirst_nocontrasts <- extract_limma_DE_results(list(eBayes_fit = limma::eBayes(fit_genderfirst_nocontrasts,
                                                                                            robust = T)))

# But still need contrasts to get at the first variable (now gender):
contrasts_genderfirst <- make_contrasts(file = "for_testing/Example Data/kaul/contrasts_genderfirst.csv", 
                                        design = design_genderfirst$design)
writeLines(contrasts_genderfirst$contrast.vec)
```


```{r}
fit_genderfirst_contrasts <- fit_limma_model(norm$normList[["vsn"]], 
                                             design_obj = design_genderfirst, 
                                             contrasts_obj = contrasts_genderfirst)
results_genderfirst_contrasts <- extract_limma_DE_results(fit_genderfirst_contrasts)
```

### Intercept model

Finally, we can do the same additive models as above, but with the intercept parameterization. We have to make the design matrix outside our package function, and do the model fitting outside our package as well. But, there is no need to set up new contrasts, as the intercept parameterization already encodes our predictors as offsets:


```{r}
design_intercept <- list(design = model.matrix(~group+gender, data = norm$targets),
                         targets = norm$targets[,c("group", "gender")])

fit_intercept <- limma::lmFit(norm$normList[["vsn"]], design = design_intercept$design)
results_intercept <- extract_limma_DE_results(list(eBayes_fit = limma::eBayes(fit_intercept,
                                                                              robust = T)))

# In this case, the code set female as the default, so need to flip some terms
# to make it male-default like the ones above
results_intercept$stats_by_contrast$genderfemale <- results_intercept$stats_by_contrast$gendermale %>%
  mutate(logFC = logFC*-1,
         CI.L.temp = CI.R*-1,
         CI.R = CI.L*-1,
         CI.L = CI.L.temp,
         t = t*-1,
         sig.PVal = sig.PVal*-1,
         sig.FDR = sig.FDR*-1) %>%
  select(-CI.L.temp)

```


## Comparing results across methods

Now that we've run the model a few ways, we can compare the results across our different specifications/parameterizations. 

### Comparing within models

First, let's look within our no-intercept, 2 factor models. For these models, we could calculate the DE effect for our second term in two ways: by looking directly at the model output, or by specifying a contrast. These should be the same. Let's check the group effect (which means looking at the gender-first model):

```{r}
all.equal(results_genderfirst_nocontrasts$stats_by_contrast$HCV_HCC,
          results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_1) 
all.equal(results_genderfirst_nocontrasts$stats_by_contrast$HCV_HCC,
          results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_2)  %>% 
  writeLines()
all.equal(results_genderfirst_nocontrasts$stats_by_contrast$HCV_HCC,
          results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_3)
```

This looks fine: the second way I tried specifying the contrast ("HCV_HCC-(male+female)/2") is incorrect, but the other two are the same, and match the effect estimated straight from the model. We can check if the same is true of the gender effect (within the group-first model):


```{r}
all.equal(results_groupfirst_nocontrasts$stats_by_contrast$female,
          results_groupfirst_contrasts$stats_by_contrast$male_vs_female_1)
all.equal(results_groupfirst_nocontrasts$stats_by_contrast$female,
          results_groupfirst_contrasts$stats_by_contrast$male_vs_female_2) %>% 
  writeLines()
all.equal(results_groupfirst_nocontrasts$stats_by_contrast$female,
          results_groupfirst_contrasts$stats_by_contrast$male_vs_female_3) 
```

And we get the same results. So, that is good: **in the no-intercept, two-factor model, we get the same DE results for the second predictor variable whether we specify the contrasts or not**.

### Comparing order across no-intercept models

Next, we want to look across our models: the order of specifying terms shouldn't matter (right?), so the group effect estimated in the no-intercept, group-first model should be the same as the group effect estimated in the no-intercept, gender-first model. Let's see:

```{r}
all.equal(results_groupfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control,
          results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_1) %>% 
  writeLines()
```

So, they are not quite the same: the estimates of LogFC and average expression are the same, but the CIs and stats are a little different. The differences aren't huge (in the 3-4 decimal place, on average), and they don't led to big differences in terms of statistical inference: only `r sum(results_groupfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control$sig.FDR != results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_1$sig.FDR)` proteins change significance of DE for adjusted P-values, and `r sum(results_groupfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control$sig.PVal != results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_1$sig.PVal)` change considering raw P-values. Still, the differences are a little odd: they're larger than we'd generally expect from floating point imprecision in R (usually in the 14-16th decimal places), so I think they suggest some way in which the stats are calculated in subtly different ways when the order is flipped. 

We can check and see if the gender effect has the same issue:

```{r}
all.equal(results_genderfirst_contrasts$stats_by_contrast$male_vs_female,
          results_groupfirst_contrasts$stats_by_contrast$male_vs_female_1) %>% 
  writeLines()

```

And it does, though again, the differences are small and for this effect they don't change our statistical inference at all: `r sum(results_genderfirst_contrasts$stats_by_contrast$male_vs_female$sig.FDR !=      results_groupfirst_contrasts$stats_by_contrast$male_vs_female_1$sig.FDR)` proteins change the significance of DE with adjusted P-values, and `r sum(results_genderfirst_contrasts$stats_by_contrast$male_vs_female$sig.PVal !=      results_groupfirst_contrasts$stats_by_contrast$male_vs_female_1$sig.PVal)` change considering raw P-values.

So, this is a little odd: **in the no-intercept, two-factor model, the order of predictor variables does have a (small) effect on statistical inference**. Perhaps comparing against our last model, the intercept model, will help us figure this out. 

### Intercept vs. no-intercept models

First, let's just check: does order matter in the intercept model? We can run it with the order flipped (with gender first) and compare:

```{r}
# Change order
design_intercept_flip <- list(design = model.matrix(~gender+group, data = norm$targets),
                         targets = norm$targets[,c("gender", "group")])

fit_intercept_flip <- limma::lmFit(norm$normList[["vsn"]], design = design_intercept_flip$design)
results_intercept_flip <- extract_limma_DE_results(list(eBayes_fit = limma::eBayes(fit_intercept_flip,
                                                                              robust = T)))


all.equal(results_intercept$stats_by_contrast$groupHCV_HCC, results_intercept_flip$stats_by_contrast$groupHCV_HCC)
all.equal(results_intercept$stats_by_contrast$gendermale, results_intercept_flip$stats_by_contrast$gendermale)
```

So, order doesn't matter in the intercept model. Let's compare the different two-factor, no-intercept models to the intercept model. First, the group predictor:


```{r}
# When group is listed first
all.equal(results_groupfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control,
          results_intercept$stats_by_contrast$groupHCV_HCC) %>% 
  writeLines()
```


```{r}
# When group is listed second
all.equal(results_genderfirst_contrasts$stats_by_contrast$HCV.HCC_vs_Control_1,
          results_intercept$stats_by_contrast$groupHCV_HCC) 
```

So, assuming the intercept model is "correct" (which may be wrong, but seems reasonable given that order doesn't matter for it), in the two-factor no-intercept models, the **second** factor is the one that is being calculated correctly. 

Can confirm by looking at gender:

```{r}
# When gender is listed first
all.equal(results_genderfirst_contrasts$stats_by_contrast$male_vs_female,
          results_intercept$stats_by_contrast$genderfemale) %>% 
  writeLines()
```


```{r}
# When gender is listed second
all.equal(results_groupfirst_contrasts$stats_by_contrast$male_vs_female_1,
          results_intercept$stats_by_contrast$genderfemale) 
```

So, why do we get slightly different CIs, t- and B-statistics, and P.Values when we specify the order differently, even though we get the same point estimates of logFC? Let's dive into the model fitting objects to try and figure this out. 

These result tables that we're comparing are just calculated/extracted from the limma eBayes fit object. So, I compared the eBayes fit objects for the two different orders of the no-intercept model element-by-element to look for differences (while accounting for differences in column order):

```{r}
# Coefficients:
# colnames differ, but otherwise the same
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$coefficients[,c(1,3)],
               fit_genderfirst_contrasts$eBayes_fit$coefficients[,c(2,1)], 
               tolerance = 1e-12)
# std.unscaled
# differences in a bunch of rows, but most the same
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$stdev.unscaled[,c(1,3)],
               fit_genderfirst_contrasts$eBayes_fit$stdev.unscaled[,c(2,1)], 
               tolerance = 1e-12)
# sigma: the same
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$sigma,
                      fit_genderfirst_contrasts$eBayes_fit$sigma, 
                      tolerance = 1e-12)
# df.residual: the same
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$df.residual,
                      fit_genderfirst_contrasts$eBayes_fit$df.residual, 
                      tolerance = 1e-12)

# cov.coefficients: have to look at manual, but the same values
#fit_groupfirst_contrasts$eBayes_fit$cov.coefficients,
#fit_genderfirst_contrasts$eBayes_fit$cov.coefficients, 

# pivot, rank, Amean, method all the same
# design and cotnrasts are different, as expected. 

# df.prior and s2.prior are the same

# var.prior are slightly different
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$var.prior[c(1,3)],
               fit_genderfirst_contrasts$eBayes_fit$var.prior[c(2,1)], 
               tolerance = 1e-12)

# proportion the same
# s2.post the same

# t statistics are only different in the same rows with different
# unscaled std devs
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$t[,c(1,3)],
               fit_genderfirst_contrasts$eBayes_fit$t[,c(2,1)], 
               tolerance = 1e-12)

# df.total the same

# p vals different, in same rows as above
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$p.value[,c(1,3)],
               fit_genderfirst_contrasts$eBayes_fit$p.value[,c(2,1)], 
               tolerance = 1e-12)

# Lods different in every row 

# F different in same rows as above
tmp <- waldo::compare(as.matrix(fit_groupfirst_contrasts$eBayes_fit$`F`),
               as.matrix(fit_genderfirst_contrasts$eBayes_fit$`F`), 
               tolerance = 1e-4)

# p-values not the same, but are all so close to 0 (e.g., 1e-70) that it doesn't
# matter
tmp <- waldo::compare(fit_groupfirst_contrasts$eBayes_fit$F.p.value,
               fit_genderfirst_contrasts$eBayes_fit$F.p.value, 
               tolerance = 1e-12)

# Get the indices of rows where things are different:
problem_rows <- tibble(a = unname(fit_groupfirst_contrasts$eBayes_fit$stdev.unscaled[,1]),
                       b = unname(fit_genderfirst_contrasts$eBayes_fit$stdev.unscaled[,2])) %>% 
  mutate(diff = abs(b -a) > 1e-12) %>% 
  pull(diff) %>% 
  which(.)
```

So, some differences are specific to certain rows (std. deviations, t-stats, and F-stats), and some (the lods) are different for every row. The priors for variance are also different. Together, all these differences lead to the very slight downstream differences (in CIs, P-values, statistics, etc). 

We can spot check the data in all these rows. In brief, we find that these are rows where some samples have missing data. So, that seems to give us the cause of the issue. In the presence of missing data, limma calculates the unscaled standard deviations slightly differently, leading to slight differences in a bunch of other stats as well. We can test this to be sure: if we remove all the rows with any missing data, we should get the same answers:

```{r}
vec_has_na <- function(vector) {
  TRUE %in% unique(is.na(vector))
}
  
no_miss <- norm$normList[["vsn"]][which(!apply(norm$normList[["vsn"]], 
                                               MARGIN = 1, 
                                               FUN = vec_has_na)), ]

fit_groupfirst_no_miss <- fit_limma_model(no_miss, 
                                            design_obj = design_groupfirst, 
                                            contrasts_obj = contrasts_groupfirst)
results_groupfirst_no_miss <- extract_limma_DE_results(fit_groupfirst_no_miss)


fit_genderfirst_no_miss <- fit_limma_model(no_miss, 
                                           design_obj = design_genderfirst, 
                                           contrasts_obj = contrasts_genderfirst)
results_genderfirst_no_miss <- extract_limma_DE_results(fit_genderfirst_no_miss)


all.equal(results_groupfirst_no_miss$stats_by_contrast$HCV.HCC_vs_Control,
          results_genderfirst_no_miss$stats_by_contrast$HCV.HCC_vs_Control_1)
all.equal(results_groupfirst_no_miss$stats_by_contrast$male_vs_female_1,
          results_genderfirst_no_miss$stats_by_contrast$male_vs_female)
```


And they are the same, as expected. So, what exactly is going on with limma such that, in the presence of missing data, it calculates slightly different standard deviations based on the order of model paramterization? I looked through the limma code to try to figure this out. It seems like, for each gene, limma fits a robust linear model with `MASS::rlm()`. The coefficients from that rlm are the coefficients that are output by limma (which are the same no matter the order), while the unscaled standard deviations are calculated with: `sqrt(diag(chol2inv(out$qr$qr)))`, where `out` is the object returned from `MASS::rlm()`, the `qr` element is the QR decomposition of the model matrix, and `chol2inv()` gets you the inverse of a matrix from its Cholesky decomposition. I can do the rlm fitting for a single gene where there is missing data, and recapitulate that we get different unscaled standard deviations:

```{r}
# genderfirst
data <- results_genderfirst_contrasts$data
y <- as.vector(data[problem_rows[1],])
obs <- is.finite(y)
design <- design_genderfirst$design
X <- design[obs, , drop = FALSE]
y <- y[obs]
w <- rep_len(1, length(y))
out <- MASS::rlm(x = X, y = y, weights = w)
sqrt(diag(chol2inv(out$qr$qr)))



# group first
y2 <- as.vector(data[problem_rows[1],])
obs2 <- is.finite(y2)
design2 <- design_groupfirst$design
X2 <- design2[obs2, , drop = FALSE]
y2 <- y2[obs2]
w2 <- rep_len(1, length(y2))
out2 <- MASS::rlm(x = X2, y = y2, weights = w2)
sqrt(diag(chol2inv(out2$qr$qr)))
```

I could try going a little further down the rabbit hole and look at the MASS code for fitting rlm, but once we get to Cholesky decomposition I feel like I've reached my limit on linear algebra for the day. 

So, to sum up this section: when using the no-intercept parameterization with multiple factors, if there's missing data than the order of the statistical factors matters for calculating statistics and results. This does seem to be a relatively minor issue, in the sense that it won't lead to wildly different CIs and p-values. 

### Additive vs interactive models

We can do one final model comparison: as I mentioned above, the 1x4 model is, implicitly, an interaction model. So, its estimates should be a little different from our three other models, which are additive. We'll compare against the intercept parameterization: 

```{r}
all.equal(results_onecol$stats_by_contrast$HCV_HCC_vs_Control,
          results_intercept$stats_by_contrast$groupHCV_HCC, ) %>% 
  writeLines()
```

Annoyingly, `all.equal()` seems to prioritize printing output on missing values first. But, there are bigger differences (in logFC, and in stats) between these, as we'd expect given that the models are different. 

## A single predictor with >2 levels

This report has focused on experiments with multiple treatment factors, but many of our experiments use designs with only one factor, but with more than one level for that factor. For those types of models, our default parameterization (the no-intercept one) makes sense, as it makes specifying contrasts much easier. But, out of curiosity: do we get the same results when we use the two different parameterizations?

To test this, I'll look at a different project I've been using for testing: the Rebello project. This project has one experimental factor, "group", with 5 different levels and thus 10 different pairwise comparisons that the researchers were interested in. First, we can load in the data:

```{r}
# Load expression data
raw_data <- extract_data("for_testing/Example Data/rebello/Samples Report of Rebello_040522.csv",
                         pipe = "DIA",
                         enrich = "protein")

targets <- make_targets(file = "for_testing/Example Data/rebello/Rebello_040522_metafile_DIA.csv",
                        sampleIDs = colnames(raw_data$data),
                        pipe = "DIA",
                        enrich = "protein")

# Subset targets
sub <- subset_targets(targets = targets,
                      filter_list = list(group = "pool"))

# Process data
norm <- process_data(data = raw_data$data,
                     targets = sub$targets,
                     min.reps = 3,
                     min.grps = 2)
```

Then, we can specify the design as we normally do, using the default no-intercept parameterization:

```{r}
design_noInt <- make_design(targets=norm$targets,
                            group_column = "group",
                            factor_columns = NULL,
                            paired_column = NULL)
```

Leading to this design matrix:

```{r}
head(design_noInt$design)
```

And this formula:

```{r}
design_noInt$designformula
```

With this design, we can use the default contrasts:

```{r}
contrasts_noInt <- make_contrasts(file = "for_testing/Example Data/rebello/contrasts.csv", 
                                   design = design_noInt$design)

writeLines(contrasts_noInt$contrast.vec)
```

And do the model fitting as normal:

```{r}
fit_noInt <- fit_limma_model(norm$normList[["vsn"]], design_noInt, contrasts_noInt)
results_noInt <- extract_limma_DE_results(fit_noInt)
```

Then, we can do the model fitting the non-default way, using the intercept parameterization. First, we need a new design matrix:

```{r}
design_intercept <- list(design = model.matrix(~group, data = norm$targets),
                         targets = norm$targets[,c("group")])
colnames(design_intercept$design) <- stringr::str_remove(colnames(design_intercept$design), "group")
colnames(design_intercept$design) <- stringr::str_replace(colnames(design_intercept$design), "\\(Intercept\\)", "Control_untreated")
```

```{r}
head(design_intercept$design)
```

We also need new, contrasts that are relative to the intercept term:

```{r}
contrasts_intercept <- make_contrasts(file = "for_testing/Example Data/rebello/contrasts_Intercept.csv", 
                                      design = design_intercept$design)

writeLines(contrasts_intercept$contrast.vec)
```

Then, we have to fit the model outside our function, to use the right contrasts:

```{r}
tmp_fit <- limma::lmFit(norm$normList[["vsn"]], design = design_intercept$design)
contrast_fit <- limma::contrasts.fit(fit = tmp_fit, contrasts = contrasts_intercept$contrasts)
efit <- limma::eBayes(fit = contrast_fit, robust = TRUE)

results_intercept <- extract_limma_DE_results(list(eBayes_fit = efit))

```


Then, we check to see if everything is equal:

```{r}
all.equal(results_noInt$stats_by_contrast, 
          results_intercept$stats_by_contrast) %>% 
  writeLines()
```

And again, things are ever so slightly not equal. It looks like the contrasts involving the control group, which was our intercept in the model, are all the same. However, the other comparisons have subtle differences. As before, these aren't in the estimates of average expression, but rather the CIs (and thus the corresponding test statistics and p-values). In this case, they don't lead to any differences in significance for individual proteins, but in theory they could.


In going through and doing some spot checking, the issue again seems to be missing data: unscaled standard deviations are different between the intercept- and no-intercept parameterizations for rows/proteins that have missing data. I won't go all the way down the stack here, but I assume this is once again due to differences in the QR decomposition of matrices during the rlm fitting. 


## Conclusions

So, some of these model specification issues make a difference. In brief:

* When there is no missing expression data, neither the parameterization nor the order of specifying factors seems to matter.

* With missing data, however, both of these factors matter. In the no-intercept parameterization with multiple factors, you will get slightly different results fitting the same model in different orders. When there is only one factor with multiple levels, the choice of parameterization will affect the calculation of CIs and statistics.

* Need to be careful when specifying 2x2 vs 1x4 (or other higher-level) designs: need to think about when we want to include interactions, and when we don't.

* As it is now, the default parameterization in our pipeline/R package is the no-intercept one. This perhaps makes specifying contrasts easier, but does mean that, for any project with missing data (which I'm sure is basically all projects), the statistics might be a little different from the intercept parameterization (which I am assuming is the "correct" one, given that it is order-invariant in the multi-factor models). These differences are small and shouldn't really affect our inference much, but they're something to keep in mind. And perhaps it would be worth it to add an option in the `make_design()` function to allow easy switching between parameterizations.  

